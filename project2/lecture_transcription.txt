 And that includes if you're struggling with that, they do have like exercises to do that you can build up skills by solving some of the exercises. If you want to, you may need to do that as part of the process to get into this project. As usual, I think you should probably start looking at this thing right away because you're in need of all the time that you can. So again, it's going to, there are some advantages to OpenMP. You're given the serial program, but you can just like focus on parallelizing one part at a time, which is different from the MPI program. So in that sense, it's going to be a little bit more straightforward. On the other hand, you can have very subtle bugs in threaded programs that can be very hard to track down. So, you know, you get some and you lose some. Okay, so let's talk a little bit about how a code like this works. So now basically the core of the solver is the computation of fluxes. So I'm just showing that flux kind of like an area arrow. And what we're going to do is we're starting with just a grid of cells and this cell is going to be in I and J and K. We call it an eye. And you're going to have a loop I, J, K variables that would loop over it. And I would be the number of cells in the eye direction and J would be the J direction and so forth. And so what's going to happen for each cell? Well, for each boundary between cells, there is what we'd call a face. And when we're modeling this, what we really need to know is how much material moved across that face. And that's called a flux. Okay. And so to compute that flux for the algorithm that we're using, we're going to need to know about the fluid state at four cells, two cells on one side of that face and two cells on the other. Okay, so you can think of it like maybe we're fitting a polynomial between those. It's actually more complicated than that. But in each cell, there's some information describing the state of the fluid. There's actually going to be four variables in each cell. Pressure and the X component of velocity, the Y component of velocity and the Z component of velocity. Okay. And the flux is going to have four terms. It's going to have a flux that's associated with the pressure equation. It's going to have a term that's associated with the X momentum equation, which is the related to the X velocity. And then one related to the Y velocity and one related to the Z velocity. Okay. So the only thing that you really need to know for this is that we have some formula that we're going to have to evaluate. It's going to take the pressure and velocities from these four cells. And from that, it can compute a flux through that face. Now it turns out these fluxes are slightly different if it's in an X going direction or Y going direction or Z going direction. So we're going to separate it out. Faces that go this way, faces that go up and down, faces that go in and out. Okay. But they're almost the same flux, but they're slightly different because one is you're going to use, say, the X component of velocity if we're talking about going in the X direction. Right. In a different way than the other components. Okay. Okay. Now what what we really need to know in terms of how this fluid cell evolves in time is we're going to need to know what is all of the material that's going in and coming out of that cell. Right. So like if there's a deficit, if there's more material that's going in and coming out, then we know that the pressure is increasing in that cell. If it's the other way, the pressure is decreasing. If there's a net, you know, extra momentum coming into the cell, then the flow there is going to accelerate. Okay. So when we compute basically the differences of all these fluxes. Oh, another thing is that this is conservative. Like any flux that gets added to this cell came from this cell. So what we're going to do is we're going to subtract this flux from this cell and add it to this cell. So for every flux we compute, there's two cells that we touch from that. Okay. And then if you combine all of these data that's accessed to compute the flux at a cell, it's going to look like this. So on 3D, it's going to have two directions like that. So it's going to look like jumping backs. Okay. Now, so again, the details of exact for your purposes, the details of exactly what computations required to compute that flux is not important, but you need to know how this data is arranged because that's going to be important for the parallel part. Okay. Now, now what what what we do after we sum those fluxes, what we have is something that's called a residual. What that residual is is really a rate of change. That's how fast the velocity or pressure or whatever is changing. It's a it's going to be a rate. So if it's pressure, how fast the pressure is changing with respect to. And so we use we're going to have to take that and integrate it to figure out how does the fluid evolve in time. So you can imagine in a simple case, if I have something that's changing at a certain rate, say it's changing, you know, increasing with by 10 per second, then if I if I'm integrated over a period of one second, I'm going to increase it by 10. Right now we're using in this case a third order rung a cut. By the way, this is a fourth order scheme. That means that when we refine the mesh, the error will go down with that refinement to the fourth power. Okay. So this is a fairly accurate scheme. And we're using a third order in time integration scheme. Third order in time will require that we evaluate residuals at three different values. So we're going in order to advance one time step, we're going to have to compute the fluxes three different times with different values for the velocity and pressure and so forth. Okay. And that time integration basically just ends up being a weighted sum of those residuals. Okay. Okay. So that just kind of gives a big picture. So now let me talk about something else. So we are dealing with a 3D array. Okay. Now, unfortunately, C and C++, although they have 3D arrays, they are 3D arrays in which the dimensions are known at compile time. They don't really have 3D arrays built into the language where the dimensions are not known until runtime. You can you can model them in different ways. But what we're going to do is we're going to actually explicitly calculate the array equation to access. So if we want to access IJK at a particular location, we're going to take a 1D array, compute its index by computing by calculating its offset in that array. Okay. Using that array equation. There are many ways of solving this problem and see it's not really standardized, but basically a C and C++ don't have a multi-dimensional array facility built in that has dynamically dimensioned. And that's just an unfortunate fact. So we're going to use this array equation. And the reason is. Okay, we're going to simulate in this box and this box is going to go from zero. Zero to let's say on this side, it's going to be. This is going to be an eye on zero. Right. This corner here would be in I, actually, we'll do this and I minus one and I minus one. So we're going to have an index where we start from zero. And so we're going to calculate it in this fashion, but we want to have periodic boundaries. So there's going to be two cells that needs to sit beyond this array. Here and here along the outer edges. So we want we're going to have to allocate space for this. So we're going to have to allocate an array that's actually larger than that by four in each to accommodate those two extra cells. And what we're going to do is we're going to in this array array equation, we're going to be adding this to so that zero becomes two, two. So we're doing that transformation. OK, so that's why our array array equation is, for example, I plus two times and J plus four and K plus four and I plus four and J plus four and K plus four is the actual size of the array that we allocate. Right. And that's just so that we can have these these extra regions along with these sometimes this is called fringe cells or ghost cells or something like that. OK. And this is this calculation basically would match kind of if we had done a three compiler defined array, it's using the same arrangement of data for that. OK. So now with this calculation, we can convert IJK directly into an array in our one dimensional array. And if we wanted to act, we can access actually minus one is a valid index array. So we can we can access this cell. The cell here would be located at minus one comma zero. But we'll never need to access. No, you will, because if I'm computing the fluxes of this stencil, that stencil is going to be like this. Now, what's going to happen because this is a periodic box, we're going to have to copy data from these cells over there. So it looks like this world wraps around when we get to the edges. OK, so that's how this that's how the boundaries of this simulation are treated. That box just periodically repeats in all directions. OK. So we'd have like negative one zero would be what we'd have at the end of the other box, like what it wraps around like that. So like this this part here would be like negative one comma zero would be its location. And there's going to be a stage where we would want to copy this. This would be N, I minus one comma zero. You want to copy this over to there. And then this cell here, you would want to copy over to there so that it looked like this was just going on to infinity, just repeating. OK, that's that's what we're going to be doing. So there will be a step when we get to the in the code, we're going to have a step called copy periodic, which its job is to just copy that data into the fringe cells. OK. OK. Yeah, these are the details. It's important for you to understand this part here. You don't need to understand the necessarily the CFP. So is there a reason we're copying it instead of just looking at the index that we're copying? Well, OK, so that's that's actually a very good question. So one way you could deal with this is you could say, oh, I'm over here on this edge. But in order to do that, you're going to have to have if statements down deep in your loop. Now, having conditionals down deep in a loop is not good. Why is that? I mean, think back to our chapter one. Well, also, it's going to affect your IELTS because you have all these branches down there in the bottom. So your architect is having to decide which way does it go? Which thing do I put in the pipeline? So it's actually better to even though you might save memory, it's going to cost a lot of time. And it's actually more because you think about there's a lot of logical cases like this case here. There's a lot of logical cases like this case here is just going to read one item from there. Right. So it's going to get really complicated. So it's easier to just copy this data. So you could do that, but you probably would end up with a slower code as a result. OK, there are other ways you could do that, which instead of calculating this, we could actually just store indexes like we could store the pre-calculated values. And then we could dispense with those cells in that case. But when we start talking about parallelizing this with MPI, this can be useful because now we know we're going to send data. You know, like we could have we could break this into multiple blocks, then create fringe cells around each block and then copy data that's part of an MPI SIN to do communication between processors. OK. OK. Now, some other little tricks that we do. Like pros. This wireless is not worth it. Is there anything that I can do to. Because it's just basically. Frozen. No small. Yeah. I'm going to talk about not working that well. They need to run that cable. Yeah. And. So. So. So. So. So one of the things that I was going to say is a lot of times in that code, we're going to be accessing like we're at I, J, K. Right. So if we're at. That going to am I going to have to. You may have to reset the transmitter to. OK. So. So suppose that I'm at this point here, we'll call this I. Common J and I'm leaving out the cave because I can't draw on 3D on a chalkboard. OK. So this part, this cell here is I minus one common J. And this one here is I minus two common J. And by the way, this what we're talking about is called a stencil. And this type of computation is common in computational fluid and computational mechanics. And it just would be that these are called stencil operations. And then this would be I say J plus one and then down here would be J minus one. So we're often having to access those things. So this is referred to the cells that have the four different edges, but not the edges themselves. Yeah. So yes. So we're talking about a cell. So we're talking about this flux here. There's kind of a bias. It's kind of skip. So in other words, we might say, OK, we're going to evaluate this space here. And so we're going to be going to I plus one, I minus one, and I minus two. But if we're at this space here, then we're. Yeah. So it is a little bit confused in that regard. And now I'm just screwed. So now I just don't have any. So. So. First, that use the symbol to long press. OK. Let's just get up on this thing. OK. Well, we'll just do it by talk for right now. OK. So what's going to happen is as we're doing this loop, there's maybe somebody could go up and see if they can get roged. Yeah. OK. So. So we're going to be basically looping over these things. And so we're going to calculate out of a given phase. So when we do this loop, because if we look at the number of cells, let's just say we're doing two cells, there's two cells, but there's three faces. Right. And so when we do this loop, whichever direction we're evaluating the flux is in, we're going to go one extra. Because actually our zero is starting here. And so we're going to zero and I then we would have gone this space in this space. Then we go one more. So we pick up that face and for each one of these faces, we're going to add to either side. OK. So we're going to add either side on here and add either side on there. OK. Now. Now. In the overall structure of this. Of this application, that's going to create some issues for the for the parallelization, because while I'm updating this space, if this space here got assigned to another thread, they most both maybe trying to add their update to this cell. So that's a race condition. So that's one of the things that you're going to have to be concerned about when you develop this parallel program. Yes. So we need to like the block wherever base in the cell for working on. OK. Yeah. So that's yes. So the question is, this is a very key question. How do we deal with this race? So one way you can do it is potentially by using lock. Yeah. It throws up on me and now I can't use it. I'm not a big fan of it. It's very lagging. So I think you want to get like the HDMI cable directly. I only problem is like it would be. I see the problem that I have with the wireless is that like I was trying to show some animations earlier. It's like, you know, it's hardly you hardly even make sense of it. And then it froze halfway through my presentation and I couldn't get anywhere with it. Do you want me to go over the adapter and see that the media because the media is simply. OK, that might work. Let me get you the adapter. OK. I'm asking for all. I'll say now. OK, so. So what's going to happen in that case? So there's different ways you could deal with that. So one way is you could try to lock it. But what you're going to find with that is that every thread is going to be trying to lock. So most of the time they're going to be updated regions that aren't overlapping. So those locks are going to be preventing you from getting any parallel execution. So you will get a correct result. Probably run slower than the serial program. There are you could use a time like there are special operations on this hardware that if you're just going to be adding something to a place in memory, you can use the atomic version. And that's usually there's still an overhead for that. That's not that much. Other strategies you could use is you could imagine if I have if I have an overall grid, I could start breaking this into little tiles. And if you'll notice, like I could do all of the like this would be called a checkerboard pattern. So I could say all the red squares, they they could update and there wouldn't be any problem. Right. So you could do the reds and then you could do the blacks. And now if a thread was assigned to each one of these squares, but you need to be careful what you hear. Now you need the threads to be assigned to specific. Right. And depending on how open and he works, that may not be straightforward. So. So at any rate, this. That would be one one of the so we have a comics. You can do something like this. Another approach is that you could instead of doing red, black, you can do all of the tiles, but you could just compute the flux that was on these faces twice. So you could have some logic where I'm on the tile and I'm on the edge instead of summing to both sides. I'm only going to sum to one side. And then the other thread would just recompute that. So now we're computing. We're doing more work. Right. Right. But we don't have to coordinate the threads because now we've broken that by not having both threads adding to the same place. But again, even any of these, we have to organize our loops such that we know that we're assigning tiles to thread so we know which faces we're going to duplicate that computation, which one we're not. OK. So there's a lot of different ways you can do that. Another way you could do it, like if I'm doing a loop and I'm summing this way, so this problem is just occurring in the I dimension. So I could just parallelize in the J and K dimensions. And basically, I would make a thread assigned to this, but because it's just summing in that direction for that flux, then I'm not going to have any conflicts. But the problem with that is after that, it goes along the other dimensions. And there's a problem with ownership of that day, you know, that first touch. So, you know, if I if I assign things so that I'm assigning threads this way, then when I try to do the same thing for the others, now I don't my thread doesn't own that data. So I have overhead for every memory access. OK, so there are different ways of solving this problem and they'll have different performance consequences. And it's actually hard to predict in advance how those will balance out. Like is the extra fluxes that we have to calculate to avoid conflicts, is that going to be more or less than the extra memory overhead if you don't own memory of the partition threads in a different way? OK, so there's a lot of different ways we can resolve this. OK, so. Let's just kind of go over the oh, by the way. We're going to the way that we're computing our indexes. We're doing this where we have our K dimension is just that is just sequential in there. So what we compute is this I skip and we compute this I skip J skip and K skip. So those numbers would just be how much offset. So the J would be just N.I. the N.I. dimension, which would be N.I. plus four. Right. The I skip would be N.I. plus four and J. I'm sorry. I'm sorry. This would be N.K. plus four. This would be N.K. plus four times N.I. plus four. OK, so and the K skip would just be one. OK, so now what that means is that if we wanted to access like, for example, here we're showing that we want to access I. We want to access, say, the J plus N item, then we can take our index, which is what the I.J.K. was, and we can just add N times J skip. And so we just skip over that part. So a lot of these stencil codes are implemented by just adding to the I.J. or K. And so all we have to do is just access with that stripe, whatever that skip is, to go plus. And if we want to do minus one, we would do index minus J skip. Yes. So for like to access I plus N, you would do index plus N times I skip. Yeah, exactly. So that's just a way. Again, we're doing this because C doesn't have a standard way to do a three dimensional array. OK, so let's talk about the main sort of things that are happening in this code. So this thing, this is called once. The rest of these things are going to be called three times for every time step, because, as I mentioned, we're going to have to evaluate the residual three times. We're going to have to evaluate the residual three times to evaluate the time integration. OK, so the first thing that we do is set initial conditions. So that this we're kind of solving this kind of standard vortex problem and the set initial conditions is just there's an analytic function for what that initial condition is. So it's just setting the pressure and velocity based on that function. So that's all that that does. So we're just going to loop over the I.J.K.s. From the I.J.K.s, we'll compute an X, Y, Z location of that point. And then there's a function we just plug that X, Y, Z into and we compute those functions. OK, then the next thing is what I was mentioning before, copy periodic. Let's see if I can make this bigger. Not like that. OK, so the next thing is copy periodic. So, by the way, also see how we are passing these in. We pass in array pointers to the T array. U is the X component of velocity. Z is the Y component of velocity. W is the Z component of velocity. That's just that's kind of the standard notation that's used in this. Then we have the N, I, J and K. That's the size of our computational grid. Right? The N, I plus four is the size of the allocated grid. And K start is just that offset to get to that zero, zero cell. OK? And then we pass in the I skip and J skip. The K skip is assumed to be one. So we don't pass that in. OK? So we have this copy periodic. Now the next thing we're going to have to do is compute the residual. But because the residual is the sum of those fluxes. So the first thing that we're going to do is we're going to have to just zero out that memory so that when we can start from zero, then we're going to add in all of the fluxes. OK? So we have this zero residual. And then on the compute residual, which I'll show you a little bit more, it's going to output these residual functions. So P Reset would be the time rate of change of pressure that's computed from these fluxes. U Reset would be the time rate of change of the U component of velocity. OK? And then you pass in these other variables. And you also we have some other parameters that we input. A is a parameter that's related to the numeric that we don't really have to get into. Mu is the viscosity. That's also going to be that's a parameter you can input to the code. But you're not going to be changing that. dx, dy, dz is based that computed from the number ni and j and k and knowing what the length of the boxes and various dimensions. And then the same parameters that we use to access the array. OK? Now, one of the things I didn't mention is when we do this time integration, there is we can't just take an arbitrarily large time step. The numeric can become unstable. So what we can what we have to do is compute what is the largest time step that we can take. So that's going to be done once per time step. So every time step iteration will call this once. And there will be three evaluations of residual associated with that. And so this just computes for each cell. It uses the velocity and pressure and so forth to compute a a speed, a time step. And it has to choose the minimum of all of those. OK, so this is going to also this is that's a reduction. So that's something that you're going to have to pay extra attention to because here we're combining all of these time steps into one time step. So it is going to take. OK, yes. Well, are all these functions already in? Yes. OK, they're already implemented. So but I'm just giving you sort of the big picture of how the code is working. So you have sort of your orientation. Integrate kinetic energy. That's the you know, I said this is a standard benchmark problem. So some of the things that like this problem has been solved with very fine grids and very accurate numeric so that we kind of know what the exact solution is. And a lot of times what we're evaluating is, OK, here's a match that you know, how much does it cost to reduce the match? Change the accuracy. So one of the things they plot is how the kinetic energy dissipate over time. So this starts with a certain amount of energy. And at the beginning, these big vortices start breaking down when to smaller vortices. And that doesn't actually consume much kinetic energy. Then the small vortices get so small, viscosity starts acting on them. Then the kinetic energy really starts to drop quickly. So this follows kind of like a standard profile. So that's that's some of the data that we get out. And so if we integrate kinetic energy, we just go over all the cells and we compute the kinetic energy in that cell and we sum it up to get the total volume. So this is going to be a sum over the entire problem. So that's another reduction. OK, now in it to implement the time integration, we're just doing a weighted sum. So we are going to have we're going to have three input arrays with weights. Actually, one of the input arrays is also an output array. OK, and all you're doing is just turn by turn, just like a vector operation. You're just multiplying that term by this weight, adding it to the other term. OK, so this routine is where all the work is done. And when we implement the time integration, we're just going to call that for each term that we're integrating. OK. So let me let's just look at this code real quick. Oh, by the way, I've also for this problem, when you just run it by default, it'll run very fast. And because you're just using the shared memory, you could do some of the early developments without concerning yourself with performance by just running it on the host node. These this should run in just like a second or so in its default configuration. When when you think you have it all working, then you're going to go and run these run strips to run it on the cluster. OK, now I do also have for the graduate students, the goal would be also implement hybrid parallelism. And that would mean that after you parallelize it with open MP, you're also going to parallelize it with MPI so that you can do both threads and MPI. And that's what I was saying for for the undergraduate students that wanted to kind of make up for for showing on your first project. You don't necessarily have to do the hybrid parallelism. But if you can implement a MPI version parallelized versus a thread version parallelized, although it's very simple to do hybrid. Once you've gotten an MPI version and open MP version, you might as well do hybrid. OK, so to do the MPI version, what you're going to do is like is really you're going to be mainly modifying that copy periodic. So copy periodic is now going to be doing communication, not just at the periodic boundaries, but any boundaries between processors. And instead of copying the data directly, you're going to have to have send and receive calls to communicate that data between processors. OK. OK, so. OK, so this is an example of kind of what this code might look like. So here we're computing, we're looping over IJK. And we're computing this index, right, which is the index that we're going to be accessing. And then we're going to be the X, Y, Z location of that point. And then we can define our components of velocity, pressure and our initial conditions of the velocity is zero. So these are just analytic functions that define that particular test case. OK, so and if you look at the periodic boundaries, so we're looking for we have separate loops for each one of those planes like the I equals constant plane. We have a loop that's going to do the copies for that. So there's going to be three loops and these are looping over only two indices. That's over one of those planes. So like I constant is moving over J and K. Right. And so you're going to have another one for the J periodic basis and the K periodic basis. OK, zero residual is very straightforward. Let's look at this compute residual. So in the compute residuals, we have three loops, one that's computing the X direction face fluxes and then the J and then the K. And what you can see here is that when we're doing the X direction, I is associated with the X direction. So I as I increases, X is increasing. And so you can see that here we're looping from zero to NI plus one. That's because we have one more face for the cells in that direction. OK, and then what you can see is what we extract out for that face is the left, left, left, right and right, right for the velocity. And you see here we did minus two times I skip minus I skip and plus I skip. So that's that, you know, where I was saying that face we have to do minus two, minus one, I, you know, I plus one. OK, and so we extract these values. And then here is where we're computing this flux for each one of those terms, P flux, U flux, V flux and W flux. And once we've computed those fluxes, then we just add them into the residual. So for each one of those fluxes, we subtract from one side and we add the other because we're just moving information from that cell to the other. OK, and that that's what happens in the in the I direction in the J direction. You're going to see that it's the J that does the plus one. OK. Another thing that I'll mention now is that when we're that when we do open MP, there's different ways that we can parallelize both of them. But one of the things that's very powerful is that open MP has a way of saying that you want to do four loops and it will partition the four loop for you. So if you have a four loop and you and you're having, you know, 20 threads, it will just do different kinds of partitions. And you can even choose like a static partition. This problem is well suited for static partitions because the amount of work associated with every index is exactly the same. So all the indexes should take the same amount of time. So if you put equal indexes on each thread, you're going to have the same workload. But one of the problems you're going to have is that even though and I times and J times and K gets large, one of those dimensions may not be large compared to the number of threads. So like let's say that we only have 32 in one dimension. We're trying to map 20 threads on there. It's not going to divide very well. So you can get a lot of threads idling because some of them got two iterations and some of them only got one. So you may want to tell it to combine loops together so that we have more iterations that we're spreading across threads. OK, and there is a annotation for when you tell open and feed to a parallel for loop. You have to have an annotation for you can tell that, OK, this we're wanting to combine these loops together when you do the parallel partition. OK, so those are some things you may want to pay attention to get good performance. You should be able to I just did a quick and dirty parallelization with open and I was able to get on the order of 60, 70 percent parallel efficiency. So you should be able to get fairly decent parallel efficiency from the open and I would think at least 50 percent. Oh, another thing I'll caution you about like some of these race conditions, like when we're adding in, they could produce very subtle errors. So you should be able to reproduce the exact same output. The code when it runs, it's going to tell you how many time steps it took, how many, you know, what the energy was. It outputs it to a file, but it also outputs it to the screen. And those should be exact to every single digit. Even one digit is different. You probably have a race condition that's causing things not to work properly. OK, because there is no explanation for this particular algorithm. One reason you might see a difference is like the order in which you sum those fluxes. If you were changing that, it might change how the roundoff error develops. Although even then, it's probably not enough that it's going to show a difference in the kinetic energy. But that would be the only reason. But most of the parallelization you do, like unless you change, like one way that could happen is say I moved that. I was doing I faces and then J faces. Maybe I swapped the order of those loops. Now I'm changing how things are being summed. That might change the result very, very, very slightly. But for your purposes, you should probably be parallelizing in a way that you're not changing the order that sums occur. So you should get exactly the same results. OK. OK, so that's the residual. And that's where you will also find if you actually evaluated the runtime, you're going to find that like 90 percent of the time is just spent computing residuals. That's where most of the time is spent in this code. Stable time stuff. It just loops over every cell. It's computing this DT and it's going to keep the minimum over all of them. And the kinetic energy is just computing the kinetic energy of each cell is just summing them up. This way it's some, as I said, it's doing this. It's basically doing this operation for each cell. So this is there is no race conditions here or anything. You just some of these loops are going to be very straightforward parallel. So maybe the first thing you do is just use open and parallelized straightforward loops and then move on to more complicated ones. You might be worried about the ownership, you know, because memory is owned by threads. So when you initialize that array, you want to make sure that the way that you told open MP to partition that is the same as other loops so that they are accessing the exact same memory. OK, so our main code, we're reading or parsing some command line arguments. So minus and is how big the grid is. And that's what we're changing when we run this for a bench for when we're running it by default. I think it's just 16 by 16 by 16, which is a very small grid. It's not very accurate, but it will run very quickly. So you can use that for your debugging. And but when we run it on the cluster, I think I'm setting it into 64, which is long enough to get reliable performance numbers, but not. It doesn't take more than a few minutes to run. And also this out file, that's the where the kinetic energy. So one of the things you can use is the dip utility and you can run with threads and without threads and output those to different files and dip the files. There should be exactly the same. If there's any difference, that means you have some kind of race condition in your code. OK, so maybe the first thing you do is just take this serial code run it and save that file. So you can use that as your reference. OK. So here there is a little bit of open MP in here. So here I just created this own P parallel. Everything inside those braces is going to run with the number of threads that you have assigned when you started the program. By the way, you can set an environment variable. O M P underscore num underscore threads to set the number of threads that you're going that you want open MP. And here it's just getting the thread ID. It's getting the number of threads from the from thread zero and writing it into non thread so it can print out what what the outcome is. We are also getting the time so that we can measure how long it takes to do this computation so we can compute speed. And then we get into the main calculation. So the first thing that we're going to do is allocate space for these variables. So we have the pressure and velocity and we also have to allocate space for the updated value while we're time stepping. And we also need space to store those residual calculations and we compute our skip functions and the starting location for the zero zero zero. And then we can go in and we just call set initial conditions. We're going to integrate the kinetic energy at the very beginning is what we output is actually the normalized kinetic energy. So it starts out at one and then it's percentage that I've lost. We output we open the file that we want to output. It is important that that's done by a single thread. So if you if you start different ways you can do open and you could have where the entire code just has X number of threads running throughout. But if that's the case you're going to have to say check open the file only from thread zero or something. Otherwise you'll have race conditions. There's also you need to pay attention for for whether the variable is private or not. So when you create multiple threads you still want to have the allocated data to be shared rather than private. Otherwise you create an array for every thread. OK so those are some details that you're going to have to pay attention to. So in our simulation time the first thing that we do is compute the stable time step and we're going to copy the periodic. So before we evaluate the residuals we're going to copy the periodic so we compute the fluxes that periodic boundaries correct. And then we we're going to do this three times. So this this set here is going to be updated three times to get complete the time step. So we copy the periodic zero residuals compute the residuals and then the integration for that step is to call that weighted sum for each one of those terms. And then we have to do it again. So at this point we're computing the next value and then we do within are doing copy periodic but now we're doing it to the next values. And we compute the residuals again and we do another update and then there's a final integration that we're computing the copy periodic for. And at that last weighted sum we're waiting some back into P. So now that the P U and B will be updated with a new time set value and we're ready for the next time step. And then that's then the final thing we're going to be doing is just integrating the kinetic energy and then writing that out to the file. And that's the end of that iteration. We just repeat and at the end because how the solution evolves changes the time step size the number of time steps required to get to our target simulation time is going to be dependent on how all these calculations. So if the calculations don't proceed in the same way then the number of time steps will be different. So like again if you made some mistake and you had a race condition one of the most obvious things that's going to tell you that's happening is because the number of time steps the code took is different. So if that's different there's no point in looking at any of the other variables you know those are different too. Okay. Any questions that's basically the architecture of the code any questions about the code architecture. So the only thing we'll be modifying would be the main CC file just like. Well there is only fluid dot CC in this case there is no helper files. Okay that case is just the main the fluid. The main thing you're going to be doing is adding these fragments for open MP. You're going to be telling it I want you to well there's lots of ways of doing it but the most straightforward way would be to tell it I want to partition these threads across these iterations. But you're going to have to be careful there are certain annotations the way variables are allocated is a variable private to a thread that means that every thread allocates its own copy or is it shared among threads. And so that that's some of the stuff that you're going to have to manage and how you manage that will kind of depend a little bit on how you what strategy you're using like I said you could create all the threads at the beginning. And then assign work to the various threads or you could do more of the four join models so that like when you're in that main loop most of the time you're a single thread but when you get to the when you get to the loops you fork do that in parallel and then you get to the main thread. And then you're going to have to keep going back together for the next week. Okay so you can do that there's lots of ways to solve this problem. The biggest challenge you're going to have is how to deal with those fluxes that the race conditions associated with updating that. And I would you might want to just have a little bit of room to you know give the figure you're probably going to experiment with that a little bit. But you could use like I said atomics you could use different partitions to avoid the race condition. You could change the looping structure so it doesn't add to both sides of the cells at all times. That's the problem that you have conditionals now down in the in the computation so that's not ideal. But any of those techniques will work. Okay. Yes. Yeah that's the same one that came with that part of the. Is the grading rubric the same as. Yes. Yeah so you're going to have to have a report you're going to have to have some kind of estimate on your performance and comparisons and all of that stuff. And does the program like the original program right now doesn't run in this that like serial. Yeah that's the serial code. But I'm giving you the serial code your jobs. Yeah. We have. When did you. I don't remember I. I don't have much time but I'm I am planning on that we're going to use this to do a GPU version after. So we need to have enough time for that. Okay. Well the will the great. Will the great for tests the test be posted today. I don't I don't know. I haven't even correlated yet. I haven't. May not get done today. Today. So I think probably most you all have an idea. How you did on the test. There on the short answer questions I tried to be a little bit generous so like if you were on the right track but maybe it wasn't technically correct. I still gave you the credit or partial credit. So a lot of times people lost credit. Anything down or something obviously wrong. So I think you probably know where you stand as far as that goes. I suspect. Do you know which questions stumped the class the most. I think a lot of you all were not really prepared for the last question. But but a lot of you all also got it. Or got pretty close. So it's I would say that's kind of a mixed bag. Generally speaking the people who performed poorly were also not very good at the short answer part. So I think you know maybe now you'll have a better idea how to prepare for the test if you weren't prepared for it. I think that the outline I did kind of telegraph what I was what my expectations were and I think that if you were preparing properly. Short answer part well I think for the most part people who did well in the short answer part did well on the other part. So. Okay. But I but yeah the plan is that your final exam would give you a chance to. Improve that outcome. But now those of you that poorly value your depending on the final exam. Any other questions. Okay let's see do we have enough time to start. I put this we don't but I did put the slides that I'm going to go over for open empty so you can look over them. We'll cover that next time. I have covered a little bit of open before. But this is going to be an opportunity for you to ask some questions. So it might be good to get started on the project so that when I cover the class you kind of have more focus questions. Because the reason I'm going over is to try to support the project. Okay. The reason I'm going over is try to support the project. Okay.